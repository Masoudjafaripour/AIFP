# ğŸš€ Adaptive Iterative Feedback Prompting (AIFP) for Obstacle-Aware Path Planning via LLMs

## ğŸ“œ Overview

This repository contains the code and dataset for the paper:

**"Adaptive Iterative Feedback Prompting for Obstacle-Aware Path Planning via LLMs."**

### ğŸ“Œ Abstract

Planning is a critical component for intelligent agents, especially in **Human-Robot Interaction (HRI)**. Large Language Models (**LLMs**) demonstrate potential in planning but struggle with **spatial reasoning**. This work introduces **Adaptive Iterative Feedback Prompting (AIFP)**, a novel framework that improves **LLM-based path planning** by incorporating **real-time environmental feedback**. AIFP prompts an LLM iteratively to generate partial trajectories, evaluates them for **collision detection**, and refines them when necessary using a **Receding Horizon Planning (RHP) approach**.

## ğŸ”‘ Key Features

âœ… **LLM-based path planning** with adaptive feedback  
âœ… **Collision-aware trajectory generation**  
âœ… **Iterative re-planning mechanism** using **Receding Horizon Planning (RHP)**  
âœ… **Handles static and dynamic obstacles**  
âœ… **Improves success rate by 33.3% compared to naive prompting**  
âœ… **Fully implemented with OpenAI's GPT-4 API**  

âœ… **A star and RRT planners in the same 2D domain**  

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ src/                   # Source code for AIFP framework
â”‚   â”œâ”€â”€ aifp_planner.ipynb              # Core implementation of AIFP
â”‚   â”œâ”€â”€ naive_llm_planner.ipynb         # A naive llm planner
â”‚   â”œâ”€â”€ astar_planner.py                # A* Planner in the grid version of same 2D domain
â”‚   â”œâ”€â”€ rrt_planner.py                  # RRT Planner in the same 2D domain
â”œâ”€â”€ results/               # Outputs of path planning trials
â”œâ”€â”€ README.md              # This README file
â””â”€â”€ requirements.txt       # Required Python dependencies
```

## ğŸ›  Installation & Setup

### 1ï¸âƒ£ Clone the repository:
```bash
git clone https://github.com/yourusername/AIFP-PathPlanning.git
cd AIFP-PathPlanning
```


## ğŸ“Š Experimental Results

| Environment         | AIFP Success Rate (%) | NaÃ¯ve Prompting (%) |
|--------------------|---------------------|--------------------|
| Single Obstacle   | **55.6%**            | 22.3%             |
| Double Obstacles  | **36.7%**            | 14.0%             |
| Random Obstacles  | **31.5%**            | 12.5%             |
| Moving Obstacle   | **48.5%**            | N/A               |
| Moving Goal       | **51.5%**            | N/A               |

âœ”ï¸ **AIFP significantly outperforms naÃ¯ve prompting**, especially in static environments! ğŸš€

## ğŸ“Œ Citation

If you use this work, please cite:

```
@article{AIFP2025,
  title={Adaptive Iterative Feedback Prompting for Obstacle-Aware Path Planning via LLMs},
  author={Masoud Jafaripour, Shadan Golestan, Shotaro Miwa, Yoshihiro Mitsuka, Osmar R. Zaiane},
  year={2025},
  Conference={AAAI LM4Planning Workshop}
}
```

## ğŸ—ï¸ Future Work

- ğŸ”¹ Extend AIFP to **3D navigation tasks**  
- ğŸ”¹ Integrate **Vision-Language Models (VLMs)** for richer environmental perception  
- ğŸ”¹ Explore **graph-based path representations** for improved trajectory optimization  

---

ğŸš€ **Star** â­ this repo if you find it useful!  
ğŸ“§ Feel free to submit issues, PRs, or suggestions.

**this repo is being updating.**

---

## ğŸ–¼ï¸ Qualitative Planner Comparisons

The following visualizations compare **LLM-based**, **learning-based**, and **classical planners** under identical startâ€“goal and obstacle configurations.  
All figures are shown at a **fixed size** with **compact captions** for consistent visual comparison.

### ğŸ¤– LLM-Based and Hybrid Planners

<p align="center">
  <div style="display:inline-block; text-align:center; margin:8px;">
    <img src="results/aifp_planner.png" width="260"/>
    <div><sub><b>AIFP</b>: Adaptive Iterative Feedback Prompting</sub></div>
  </div>
</p>

---

### ğŸ§  Learning-Based Planners

<p align="center">
  <div style="display:inline-block; text-align:center; margin:8px;">
    <img src="results/q_learning_planner.png" width="260"/>
    <div><sub><b>Q-learning</b>: Handcrafted reward</sub></div>
  </div>

  <div style="display:inline-block; text-align:center; margin:8px;">
    <img src="results/q_learning_planner_llm_rew_fn.png" width="260"/>
    <div><sub><b>Q-learning</b>: LLM-designed reward</sub></div>
  </div>
</p>

---

### ğŸ“ Classical and Sampling-Based Planners

<p align="center">
  <div style="display:inline-block; text-align:center; margin:8px;">
    <img src="results/astar_planner.png" width="260"/>
    <div><sub><b>A*</b>: Grid-based shortest path</sub></div>
  </div>

  <div style="display:inline-block; text-align:center; margin:8px;">
    <img src="results/rrt_planner.png" width="260"/>
    <div><sub><b>RRT</b>: Sampling-based planner</sub></div>
  </div>

  <div style="display:inline-block; text-align:center; margin:8px;">
    <img src="results/mcts_planner.png" width="260"/>
    <div><sub><b>MCTS</b>: Tree search planner</sub></div>
  </div>
</p>

---

### ğŸ” Observations

- **AIFP** refines trajectories via iterative feedback and collision checking.
- **LLM-informed reward learning** biases learned paths toward smoother, goal-directed behavior.
- **Classical planners** offer strong geometric baselines but lack semantic adaptability.

All planners are evaluated in the same 2D environment with identical obstacle layouts.
